We here elaborate on the bounds of the greedy and sampling strategies as described in Algorithms \ref{alg:greedy} and \ref{sampling}, when applied to the problems we consider in this work.

In the greedy case of Algorithm \ref{alg:greedy}, the bound analysis is similar to LTM-$k$ and based on the analysis of the greedy approach to general maximum $k$-coverage \citep{max_k_set_cover1, max_k_set_cover2}. Greedy search optimizes its result in each iteration, given the solutions from the previous iterations, until $k$ solutions have been found. Hence, this algorithm follows the general maximum $k$-coverage strategy and the factor $\frac{\textit{greedy}}{\textit{optimal}} \leq 1-\frac{1}{e}$ applies.

The key assumption highlighted by \citet{max_k_set_cover1} is that for a set selected in the $i^{th}$ iteration, no data point can be counted twice, i.e., in two different sets, towards the maximal weight summary. That is, if a data point is present in the sets selected in previous iterations, then the total weight should be the same as if we excluded this data point from one of the sets; the solutions should be non-overlapping. This is the case for the problems we consider (tiling, discriminative rule mining, tiling-based matrix diagonalization, etc).

Furthermore, the maximum $k$-set cover bound is very general and in practice, the greedy solution is way closer to the optimum than suggested by the bound. For example, \citet{GunsNR13} presented a constraint model of optimal tiling and based on this it was possible to compute the optimal $k$-tiling for $k=2$ for the smallest datasets and for $k=3$ for two datasets (a time-out was set to 6 hours). The average difference between optimal and actual area across all datasets was around $1\%$, while the bound suggests that it might deviate up to $36.7\%$.

The sampling strategy of Algorithm \ref{sampling} follows the approximate greedy scheme: on each iteration it greedily finds an approximation of a `greedily optimal' solution. As has been shown by \cite{max_k_set_cover1}, the approximate greedy schema with approximation parameter $\beta$ (i.e., a solution found at $i$-th iteration is within a $\beta$ constant factor of the greedy optimal one at $i$-th iteration) is within $1-\frac{1}{e^\beta}$ from the optimal solution.

$\alpha$, the parameter controlling the number of columns to sample, influences both runtime and coverage, i.e., solution quality. It is, however, extremely hard---if not impossible---to analytically derive the dependency between sample size $\alpha$ and approximation factor $\beta$, given the generality of the method. Many existing methods used to obtain sampling bounds rely on a) (model) distributions, or its forms, on the parameters and/or values (observed and/or latent) in the data, b) the structure of the constraints, c) the form of the optimization function. In our case, we consider a very generic sampling method and therefore determining an exact analytical form of the dependency between $\alpha$ and $\beta$ is beyond the scope of this paper.

It is possible, however, to empirically estimate the parameter $\beta$ for a particular choice of $\alpha$. We use the following approach: we estimate the average $\beta$ for a particular value of $\alpha$ as the mean of $n$ iterations. In the following the upper index denotes the iteration: $ \hat{\beta} = \frac{\sum_{i} \beta^i}{n} $. Then, to estimate $\beta^i$ for an arbitrary iteration $i$, we assume that we have computed an optimal solution $G^i$ greedily and an approximation $S^i$ via sampling as $\hat{\beta^i} = \frac{S^i}{G^i}$.

An empirical analysis using this approach can be found in Appendix \ref{appendix:extra_experiment_with_alpha}.
