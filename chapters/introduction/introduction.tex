\chapter{Introduction}\label{ch:introduction}
\epigraph{
- Homer, is it the way you pictured PhD life?\\
- Yeah, pretty much, except we drove in a van solving mysteries.
}{Could be Homer Simpson}

\begin{addmargin}[5em]{5em}
This thesis links the research fields of data mining, constraint learning
and logic programming. We introduce each of them in turn and provide an overview of the
contributions of the thesis and
its general structure.
\end{addmargin}

\section{Data mining}
The ability to write down a model and to simply push a button to get the
answer is an appealing idea. It is one of the holy grails of
Artificial Intelligence, called Declarative Programming, where one
specifies what the problem is and not how to solve it. Today in the age of data, we would like
to say what we are looking for, feed the data in and, as a result,
(almost magically) find new insights into the data. What we do not really want is to write
a lot of tedious code. Unsurprisingly, this is hard. 


The \textit{knowledge} extracted from the data can manifest itself in
multiple forms. For example, an interesting piece of data,
\textit{typically a substructure}, is referred as a \textit{pattern} \parencite{han_book}. 
The most known pattern learning task is the
\textit{frequent pattern mining}, or pattern mining for short \parencite{survey_han}. 
At a high level it can be formalized as follows: 
let $D$ be a dataset, $P$ be the language of allowed patterns and
$\phi$ be a predicate of
interest, the pattern mining problem is the problem of finding
all patterns $p$ in $P$ such that $\phi(p,D)$ holds.
Simply speaking, we enumerate objects that have a certain property of
interest.
Various measures of relevance or ``interestingness'' of a pattern exist \parencite{tias_topk}.


Many Data Mining problems, and especially pattern mining
problems, are \textbf{combinatorial search problems}, which in turn have two main components to them, namely problem modelling and search: 

\begin{center}
  Combinatorial problem = Model + Search
\end{center}

Therefore, it seems natural to model pattern mining problems 
using systems designed to perform search in declarative manner, such
as \acrlong{cp} \parencite{handbookcp} or \acrlong{asp}
\parencite{whatisasp}. %(which we discuss later in detail).

\section{Constraint programming} 
\acrlong{cp} is a declarative methodology for solving combinatorial
problems. The essence of this methodology is in writing a set of
constraints, such as $X + Y > Z$, and finding a solution to them, such
as $X = 1, Y = 2$ and $Z = 0$. This set of constraints is referred as
a \textit{specification} or a \textit{model}; for this, multiple \acrlong{cp}
languages exist, such as MiniZinc \parencite{minizinc} or Essence
\parencite{essence}. A \textit{solution} for
a model is an assignment, as for example, the one we just indicated, that satisfies
\textit{all} constraints. This general methodology is declarative,
since a user writes a set of constraints corresponding to the
properties of her problem and a \acrshort{cp} solver is looking for a
solution: this clearly demonstrates that it is a burden of the solver
to figure out \textit{how} to find a solution, while it is the user's
responsibility to specify \textit{what} the problem is in terms of
writing these constraints.

\section{Constraint learning}
Constraint Learning is the research field concerned with finding
constraints in the data \parencite{constraint_learning,QUACQ,Conacq}.
%Let us outline in simple terms, what constraint learning is about.
Similar to the pattern matching problem, we can define 
the constraint learning problem as follows:
%Explaining what constraint learning in term of the previous section:
let $D$ be a dataset, $C$ be a set of interesting properties (constraints), and
$\phi$ be a satisfaction function, 
then the problem is to find all properties (constraints) $c$ in $C$ such that $\phi(D,c)$ is satisfied.

In case of pattern mining, we enumerate objects on which the property
of interest holds: we find graphs or sequences that occur often enough
in the data and have certain nice topological properties (for example, being a
cyclic graph or have ascending characters). And in constraint learning we find interesting
properties that hold in the data (or its parts): given a set of
characters, for example, we discovered that the "ascending constraint" holds, i.e.,
characters are sorted or we found that givens graphs are all cyclic.
Therefore, we consider constraint learning and pattern mining to be interconnected, and both problems play a central role in this thesis.

\section{Logic programming}
In his seminal work Robert \textcite{kowalski} proclaimed:
\begin{center}
  Algorithm = Logic + Control.
\end{center}

%What he meant is quite simple, 
The intuition behind this statement is that
you can turn logical rules into a computational process. 
Why is this important for us? Logic deals with
relations and relational structures, it is naturally designed to
work with relational data. Therefore, if we can combine logic
with computation, we obtain a computational engine that can work in
our relational learning setting.

Consider an example in which there are three people: Ronald, Peter and
Alina. We know that people who are not allergic to fish and not vegans like sushi; 
furthermore, we know that Peter is allergic to fish. 
Who likes sushi then? We can write this puzzle as a
simple logical implication:

\begin{equation*}
    \begin{aligned}
& \textit{likes\_sushi}(X)~{:}{-}~\textit{person}(X),~\text{not}~
        \textit{allergic}(X), ~\text{not}~\textit{vegan(X)}. \\
&       \textit{allergic(peter)}. \\
&       \textit{vegan(ronald)}.
    \end{aligned}
\end{equation*}

The following notation in Prolog (Programming in logic)
\parencite{prolog_original} mimics the logical implication:

\begin{equation}\label{eq:sushi}
  \forall X: \textit{person}(X) \wedge \text{not }
    \textit{allergic}(X) \wedge \text{not } \textit{vegan}(X)
  \implies \textit{likes\_sushi}(X)
\end{equation}
Therefore, we can think of Logic Programming as executable Logic, that
we can use to model and solve relational problems.

In this thesis we use modern Logic Programming engines, such
as \acrlong{asp} (\acrshort{asp}) \parencite{ASPbook,whatisasp} and
\acrlong{idp} (\acrshort{idp})
\parencite{idp} %(we explain them in details in the following chapter)
that naturally support constraints and various types of logical
inference.

\section{Relational learning}
\sergey{TODO: finish}

\section{Contributions}
Given the abundance of relational data (such as spreadsheets, graphs
or logical structures) and of various machine learning and data
problems, a natural question arises: \textit{``How can we model
relational data mining problems?''}. This thesis introduces a number
of relational data mining problems and offers a uniform view on them.
Since the data and problems are relational, we approach them using
relational methods, such as logic programming. 

The \textbf{main contribution} of this thesis is in the novel problems
it offers and in the application of modern logic programming
        languages, such as \acrshort{asp} and \acrshort{idp}, to them. Concretely, the thesis investigates the following questions:

    \sergey{Could you be more specific on what should be corrected? They reflect
        the papers content and the general theme of relational data
        mining modelling}
                       
\begin{itemize}
  \item \cone: What are the challenges and advantages of generalizing
    classic data mining problems, such as the Boolean Matrix
    Factorization problem, into the relation setting?
  \item \ctwo: How can the constraint learning problem be formulated
   and modelled in the relational setting, where data is
   represented as spreadsheets, i.e., connected relations, and constraints are
   tabular Excel-like formulae?
  \item \cthree: Given a relational model, represented as a set of logic
    programming rules, can we mark some parts of it such as atoms,
    operators or constants to be uncertain and reconstruct the model
    using a number of positive and negative examples?
  \item \cfour: 
%  Similarly to unstructured constraint-based itemset mining: 
    How can we apply declarative relational mining
    techniques, such as logic programming, to structured frequent pattern mining, such as sequence, graph
    and query mining?
\end{itemize}

Addressing \cone, we look into the classical problem of matrix
factorization. We demonstrate how the problem of Boolean Matrix
Factorization can be generalized into relational setting, called
\textit{Relational Data Factorization}, and
demonstrate how it allows to model a variety of classic data mining
problems and novel tasks as well.

With regards to \ctwo, we look into a more practical setting.
Typically, machine learning and data mining algorithms make the i.i.d.
(independent identically distributed) assumption, which of course
breaks in the relational setting. If we look at a spreadsheet, it 
typically consists one or more tables with relationships between them or
within the table itself, usually
in a form of a formula, a macro or of some other constraint (e.g., a
C\# or VB script). If we
want to detect these connections between tables or their parts, we
have to make the model able to work with spreadsheet data, 
formulae and tabular constraints. This setting, referred to as the
\textit{Tabular constraint learning}, breaks the standard
machine learning and data mining algorithms and 
makes the problem essentially relational.

Addressing \cthree: assume we already have a logic programming or a
constraint model of a relational
data mining problem and due to relational dependencies in the data we are
forced to give up the i.i.d. assumption and, as a consequence,
standard techniques. When something changes over time or due to changes in the specification, we would like
to be able to indicate uncertainty in some parts of the model. These
uncertain or, \textit{open}, parts are called \textit{sketched} and
the whole sketched model is referred to as a \textit{sketch} and the related
problem as \textit{constraint sketching}. We propose the problem of and
a learning technique for \acrlong{skasp}.


Finally, with respect to \cfour, if we look at the area of itemset mining, where \acrlong{cp}
has been successfully applied, then the natural question to ask is:
if \acrshort{cp} has been successful for unstructured, or
non-relational, pattern mining, can we model its \textit{structured},
or \textit{relational} version, using logic programming? To answer
this question we look into \textit{sequence}, \textit{graph} and
\textit{query mining} and attack them using logic programming and
hybrid approaches based on logic programming.


\section{Structure of the thesis}
\textbf{Chapter} \ref{ch:background} introduces background information
on Logic, Answer Set and Programming and Pattern Mining.

\textbf{Chapter} \ref{ch:ReDF} presents the novel problem of Relational Data
Factorization. It demonstrates how this problem generalizes various
data mining problems such as Database Tiling and Boolean Matrix
Factorization. We start with the most basic setting and demonstrate
how various problems can be modelled within the framework by adding
or modifying the constraints. Then, we show how the framework can be
used for new and interesting applications and propose Answer Set
Programming as a method to solve the problem in the general case.
For each problem we provide extensive experimental evidence,
including solver parameter tuning. The chapter consists of the
research previously published in the following paper:

\begin{addmargin}[2em]{2em}

Sergey Paramonov,  Matthijs van Leeuwen, Luc De Raedt: Relational data
factorization, Machine Learning, Springer, December 2017, Volume 106,
    Issue 12, pp 1867–1904.

\end{addmargin}



\textbf{Chapter} \ref{ch:TaCLe} introduces  the novel problem of
Tabular Constraint Learning and the system, called TaCLe (pronounced
as the word ``tackle''). The problem setting is straightforward to
explain: we have an Excel file with formulae. The file is imported as
CSV. The formulae are lost. Can we reconstruct them? We can already 
see that the problem setting is different from the standard machine
learning problem. In a spreadsheet, columns are no longer variables
and rows are no longer records. Textual and numeric data are mixed.
Spreadsheet functions, such as Fuzzy-lookup, are unorthodox and unseen in
the constraint programming and constraint learning research communities.

The chapter consists of the research previously published in the following papers:

\begin{addmargin}[2em]{2em}
Samuel Kolb (*), Sergey Paramonov (*), Tias Guns, Luc De Raedt:
  Learning constraints in spreadsheets and tabular data. Machine
  Learning 106 (9-10), 2017.


Sergey Paramonov (*), Samuel Kolb (*), Tias Guns, Luc De Raedt:
TaCLe: Learning constraints in tabular data. 
 Proceedings of the 2017 ACM on Conference on Information and
    Knowledge Management, CIKM 2017, pages 2511-2514, Sheridan
    Communications, Conference on Information and Knowledge Management
    (CIKM), Singapore, 6-10 Nov 2017.
\end{addmargin}


\textbf{Chapter} \ref{ch:sketching} introduces the novel problem of
Sketched Answer Set Programming. The idea of sketching takes root in
imperative programming, when a function in C or Java has a part of it,
typically a constant, left unspecified. A user then provides a number
of examples for this sketched constant to be filled in with a
correct value satisfying the examples. Inspired by this approach, we
introduce Sketched Answer Set Programming, where a user writes a
regular program but with certain parts such as constants, predicates
or operators are left uncertain. Then, we rewrite this sketched
program into a regular \acrlong{asp} program, i.e., we use a standard ASP solver
to complete sketches.


The chapter consists of the research previously published in the following papers:
\begin{addmargin}[2em]{2em}
  Sergey Paramonov, Christian Bessiere, Anton Dries, Luc De Raedt:
  Sketched Answer Set Programming. CoRR abs/1705.07429, 2017.
\end{addmargin}


\textbf{Chapter} \ref{ch:StructuredMining} introduces a logic
based approach to structured pattern mining. The idea to apply general
solvers to pattern mining takes root in the work of
\textcite{declrativeapproach}, where Constraint Programming is applied to
Itemset Mining. Contrary to itemset mining, where objects do not have
any particular structures, structured mining studies mining patterns
in sequences, trees, graphs, etc. First, we introduce a general
mathematical model of graph mining based on the logical formulation of
the constraints. Second, we show that the model can be hybridized by
using our generic framework together with specialized solvers
developed for particular pattern mining problems.

The chapter consists of the research previously published in the following papers:
\begin{addmargin}[2em]{2em}
Sergey Paramonov, Matthijs van Leeuwen, Marc Denecker, Luc De Raedt:
An Exercise in Declarative Modeling for Relational Query Mining.
Lecture Notes in Computer Science, volume 9575, pages 166-182,
    Springer, Inductive Logic Programming, Kyoto, 20-22 August 2015.

Tias Guns, Sergey Paramonov, Benjamin Negrevergne: On declarative
    modeling of structured pattern mining, AAAI Workshop on
    Declarative Learning Based Programming, Phoenix, Arizona USA, 13
    February 2016.

Matthias van der Hallen, Sergey Paramonov, Michael Leuschel, Gerda
    Janssens: Knowledge representation analysis of graph mining,
    Bogaerts, Bart; Harrison, Amelia (eds.), Proceedings of ASPOCP
    2016, pages 55-76, Workshop on Answer Set Programming and Other
    Computing Paradigms, New York, 16 October 2016.

Sergey Paramonov, Chen Tao, Tias Guns: Generic mining of condensed
    pattern representations under constraints, Young Scientist's
    Second International Workshop on Trends in Information Processing
    (YSIP2), pages 168-177, CEUR Workshop Proceedings, Young
    Scientist's International Workshop on Trends in Information
    Processing, Dombai, Russia, 16-20 May 2017.

Sergey Paramonov, Daria Stepanova, Pauli Miettinen:
Hybrid ASP-Based Approach to Pattern Mining.  
Rules and Reasoning - International Joint Conference, RuleML+RR 2017,
    Springer Lecture Notes in Computer Science (LNCS), pages 199-214,
    Springer, 2017, RuleML+RR 2017: International Joint Conference on
    Rules and Reasoning, July 12-15, London, UK.
\end{addmargin}

\textbf{Chapter} \ref{ch:conclusions} summarizes the thesis and
discusses future work possibilities.

\section{Datasets, code and experimental results}
In order to make results repeatable and to follow the Open Source
spirit, we have opened and published the datasets, use-cases,
meta-data and other useful information.
They can be found in the following GitHub repositories:
\begin{itemize}
\item \url{https://github.com/SergeyParamonov/TaCLe}
\item \url{https://github.com/SergeyParamonov/sketching}
\item \url{https://github.com/SergeyParamonov/LGM}
\end{itemize}
\cleardoublepage
% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
