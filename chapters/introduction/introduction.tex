\chapter{Introduction}\label{ch:introduction}
\epigraph{
- Homer, is it the way you pictured PhD life?\\
- Yeah, pretty much, except we drove in a van solving mysteries.
}{Could be Homer Simpson}

\begin{addmargin}[5em]{5em}
This thesis links the research fields of data mining, constraint learning
and logic programming. We introduce each of them in turn and provide an overview of the
contributions of the thesis and
its general structure.
\end{addmargin}

\section{Data mining}
The ability to write down a mathematical model and to simply push a button to get the
answer is an appealing idea. It is one of the holy grails of
Artificial Intelligence, called Declarative Programming, where one
specifies what the problem is and not how to solve it. Today in the age of data, we would like
to state what type of knowledge we are looking for, feed the data in and, as a result,
(almost magically) find new insights into the data. What we do not want is to write
a lot of tedious code. Unsurprisingly, this is a hard challenge. 


The \textit{knowledge} extracted from the data can manifest itself in
multiple forms. For example, an interesting piece of data,
\textit{typically a substructure}, is referred to as a \textit{pattern} \parencite{han_book}. 
The most known pattern learning task is 
\textit{frequent pattern mining}, or pattern mining for short
\parencite{survey_han}. \pubrev
At a higher and more general level, often referred as \textit{theory
mining}, \pubrevend it can be formalized as follows: 
let $D$ be a dataset, $P$ be the language of allowed patterns and
$\phi$ be a predicate of
interest, the pattern mining problem is the problem of finding
all patterns $p$ in $P$ such that $\phi(p,D)$ holds.
Simply speaking, we enumerate objects that have a certain property of
interest.
Various measures of relevance or ``interestingness'' of a pattern exist \parencite{tias_topk}.


Many Data Mining problems, and especially pattern mining
problems, are \textbf{combinatorial search problems}, which in turn have two main components, namely problem modelling and search: 

\begin{center}
  Combinatorial problem = Model + Search
\end{center}

Therefore, it seems natural to model pattern mining problems 
using systems designed to perform search in a declarative manner, such
as \acrlong{cp} \parencite{handbookcp} or \acrlong{asp}
\parencite{whatisasp}. %(which we discuss later in detail).

\pubrev

\begin{figure}[thb]
  \begin{center}
    \includegraphics[width=1\textwidth]{declarative.png}
  \end{center}
  \caption{A schematic depiction of the declarative approach to data mining}
  \label{fig:declarative_schema}
\end{figure}


The declarative approach can be schematically depicted in Figure \ref{fig:declarative_schema}, the key idea here is the following: when we deal with the imperative approach, even a slight change in the specification of a problem typically leads to a significant change in the implementation of the algorithm solving this new problem variation. Contrary to that in the declarative paradigm, we translate problem specification into an executable declarative language that can handle various rules and constraints, as a result, a slight change in the specification translates, ideally, into an additional constraint being added or a constraint being changed. The key advantage of this approach is that we solve the whole class of problems instead of solving its particular version. In our setting, we can think data mining problem formulated using mathematical constraints as a specification and solver being \acrshort{cp} or \acrshort{asp} engines being solvers, then the interface is a translation between constraints specified as formulae and logical rules or constraints, as their executable formulations.
\pubrevend


\section{Relational and logical learning}
Typical machine learning and data mining systems work with
propositional representations and logical and relational learning has
arisen to overcome this limitation by focusing on expressive knowledge representation languages such as first order logic \parencite{luc_book}.

\pubrev
\begin{figure}[thb]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{fisher.png}
  \end{center}
  \caption{the famous fisher's dataset with first three columns being numeric properties of irises and the last column being its class. a typical problem then is to predict the class in the last column based on the given numeric values in the first three columns}
  \label{fig:iris}
\end{figure}


To illustrate this idea, let us consider one of the most classic datasets in statistics and machine learning -- Fisher's Iris data set\footnote{\url{https://en.wikipedia.org/wiki/Iris_flower_data_set}}. In Figure \ref{fig:iris}, we demonstrate a sample of data in the dataset. One of the most common tasks in machine learning is of the supervised learning: can we train a model such that it can predict the Iris class based on its numeric properties (in the first three columns). Looking carefully at this data, we can establish the following: 1) it is single table data 2) rows are independent of each other 3) we are looking for a function mapping an $R^n$ into $\{-1,0,1\}$ (or into probabilities of being in a particular class, etc) 

Contrary to this classic machine learning setting, relational learning is looking into a more complex and interconnected relational data. A real-world example and, probably very well familiar to a reader, a \textit{spreadsheet} with its formulae is an
excellent example of relational data: it contains a number of
relations or \textit{tables}, and their number is never fixed in
advance, since a spreadsheet might contain an arbitrary number of
them. Also, it combines both textual and numeric data. Furthermore,
formulae and constraints over the tables play a role of relationships.

\begin{figure}[thb]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{spreadsheet.png}
  \end{center}
  \caption{A spreadsheet is an example of relational data: it has multiple connected tables, constraints between tables, formulae over columns and rows, numeric and non-numeric entries}
  \label{fig:spreadsheet_example_intro}
\end{figure}

Figure \ref{fig:spreadsheet_example_intro} demonstrates this idea: there are multiple tables connected to each other, the table on the right has aggregated data over rows in the left table, which means that generally speaking rows cannot be seen as independent: adding a row to the first table should update the values in of the right table; there are various data types present here numeric, categorical and might have been textual as well. Furthermore, constraints that hold in a spreadsheet do not have to be functional, i.e., having a form of $f(\bar x) = y$, where given input $\bar x$ you can always compute $y$. An example of a non-functional constraint, as we called it here, is a foreign key: simply speaking a foreign key constraint enforces the subset relation: in Figure \ref{fig:spreadsheet_example_intro}, we see that the right table has an aggregated information over the left table, namely a \texttt{SUMIF} \footnote{\url{https://support.office.com/en-us/article/sumif-function-169b8c99-c05c-4483-a712-1697a653039b}}, then there is a subset relationship between the column \textbf{Product} in the right table and in the left -- it cannot be the case that aggregated information contains a product not listed in the table on the left.
\pubrevend


Classical machine learning algorithms such as the induction of decision trees
\parencite{decision_trees} or Bayesian networks \parencite{pearl} work
with limited representations (which correspond to a
propositional language). This representation does not allow to
represent naturally problems involving multiple complex objects
together with the relationships that hold between them. This limitation has been
pointed out by a number of researchers, e.g., in the seminal work of
\textcite{plotkin}, who has introduced  the use of first order logic
for machine learning. This approach allows to
represent a non-fixed amount of complex objects together with their
relationships -- we refer to this problem setting as \textit{relational}.

Let us indicate why it is important to look into the relational
machine learning setting. To do so, we will describe a relational problem where
the number of entities and relationships between them is not fixed,
which makes it complicated if not practically impossible to employ the
standard machine learning techniques based on  fixed feature vectors
or attribute-value representations, while using logical and relational models appears natural and elegant. 

\pubrev
A schedule is another example of relational data, which often involves complex constraints.
\pubrevend

\begin{example}[Examination room scheduling]
To make this example concrete, assume that students need to be assigned to
auditoria for examinations with professors. Typically, making such a
schedule involves dealing with interconnected constraints:
    \begin{itemize} 
    \item certain exams
must have enough gap time between them 
    \item rooms can be shared by at most
    a few courses 
\item professors might have their own personal preferences
\item university administration might enforce certain administrative and 
legal constraints 
    \end{itemize} 
    
A possible approach is to learn these 
constraints by recovering them from
a set of positive and negative examples. The number of constraints is
not set in advance and the constraint language is typically a rich
relational language, for example, a constraint saying that no room is
shared by more than three courses at the same time can be expressed in the first
order language as
\begin{equation*}
    \forall X,T: \textit{room(X)} \wedge \textit{time(T)} \implies \{ C : \textit{examination}(X,C,T) \wedge \textit{course(C)} \} \leq 3.
\end{equation*}
\end{example}

These properties of interconnectedness and expressivity of the
constraints make the
problem hard to model using traditional propositional approaches,
while this can be clearly seen as a first order problem. This gives us
confidence that for a number of important problems (such as recovering formulae in spreadsheets, factorizing relational data and mining frequent structures in relational data) it is essential to
look into richer knowledge representation languages for machine
learning.

\section{Logic programming}
\pubrev
In Figure \ref{fig:declarative_schema}, we indicate that \textit{a solver} is going to take care of computing a specification describing the problem. Also, we already know that \textbf{Data} in this schema is relational, i.e., represented as relations. We know that \textit{Logic} naturally can represent relations and relational data in general, then it is logical and natural to use \textit{Logic Programming} as the solving technique in our declarative approach.
\pubrev

In his seminal work Robert \textcite{kowalski} proclaimed:
\begin{center}
  Algorithm = Logic + Control.
\end{center}

%What he meant is quite simple, 
The intuition behind this statement is that
you can turn logical rules into a computational process. 
Why is this important for us? Logic deals with
relations and relational structures, it is naturally designed to
work with relational data. Therefore, if we can combine logic
with computation, we obtain a computational engine that can work in
\pubrev
our relational learning setting: computationally it means that an executable model we obtain, following the schema in Figure \ref{fig:declarative_schema}, is close to the original representation of the data and to the specification of the problem.

Ideally, we would like to have such a high level logic programming language that we can write human-like specifications and directly execute them in the system \parencite{denecker}.
\pubrevend

Let us demonstrate how logic programming can be used to model and solve problems. 
\begin{example}[Modelling Sushi preferences with logic]
There are three people: Ronald, Peter and
Alina. We know that people who are not allergic to fish and not vegans like sushi; 
furthermore, we know that Peter is allergic to fish and Ronald is
    vegan. 
Who likes sushi then? We can model this puzzle as a
simple logical implication:

\begin{equation*}
    \begin{aligned}
& \textit{likes\_sushi}(X)~{:}{-}~\textit{person}(X),~\text{not}~
        \textit{allergic}(X), ~\text{not}~\textit{vegan(X)}. \\
&       \textit{allergic(peter)}. \\
&       \textit{vegan(ronald)}.
    \end{aligned}
\end{equation*}

The following notation in Prolog (Programming in logic)
\parencite{prolog_original} mimics the logical implication:

\begin{equation}\label{eq:sushi}
  \forall X: \textit{person}(X) \wedge \text{not }
    \textit{allergic}(X) \wedge \text{not } \textit{vegan}(X)
  \implies \textit{likes\_sushi}(X)
\end{equation}
\end{example}
Therefore, we can think of Logic Programming as an executable Logic, that
we can use to model and solve relational problems.

In this thesis we use modern Logic Programming engines, such
as \acrlong{asp} (\acrshort{asp}) \parencite{ASPbook,whatisasp} and
\acrlong{idp} (\acrshort{idp})
\parencite{idp} %(we explain them in details in the following chapter)
that naturally support constraints and various types of logical
inference.

\pubrev
\acrshort{asp} and \acrshort{idp} are declarative logic programming formalisms. Contrary to Prolog, which is essentially a full programming language based on logical inference and provides full control on the execution flow, both systems fully decouple  a problem specification from the execution flow. In this sense they are closer to \acrlong{cp} and to Constrained Logic Programming. This makes them an ideal candidate for our general solver in the schema in Figure \ref{fig:declarative_schema}, since both of them support relational data natively and can solve problems declaratively based on a form of the first order logic specifications. We often refer to both systems as \textit{modern logic programming}.

Let us briefly demonstrate the power of \acrshort{asp} on the graph colouring problem (or map coloring problem, as it is known in the popular culture) \parencite{ASPbook}. We assume that \texttt{node/1} encodes nodes of the input graph and \texttt{edge/2} encodes edges of the given graph. First, we declare colours to be used for graph colouring as \texttt{colour(red). colour(green). colour(blue).}, the colors to be used in the \texttt{colouring/2} predicate, then we introduce the following constraint (called a \textit{choice rule}):
\begin{verbatim}
1 {colouring(X,C) : colour(C)} 1 :- node(X).
\end{verbatim}
The rule states that for each node \texttt{X} the predicate \texttt{colouring(X,C)} has one and only one atom to be true for values of \texttt{C} among the colours defined in the predicate \texttt{colour(C)}, simply speaking, this constraint enforces a one-to-one mapping from the nodes (the countries) to the colours.

Then, we need to define a constraint that is saying what mapping is acceptable, in \acrshort{asp}, it is done by defining what is NOT an acceptable solution:
\begin{verbatim}
:- edge(X,Y), colouring(X,C), colouring(Y,C).
\end{verbatim}
If there is an edge between two nodes (countries) \texttt{X} and \texttt{Y} and they are mapped to the same colour \texttt{C}, then it is not a valid solution. As a result, we obtain solutions, where all neighboring nodes of a graph (or neighboring countries) have different colours.
\pubrevend

\section{Constraint programming} 
\pubrev
\acrlong{cp} is a declarative methodology for solving combinatorial
problems that also can be used in the schema from Figure \ref{fig:declarative_schema}. The key difference is that logic programming is typically focused on rules and inference, while \acrshort{cp} is solely focused on constraint satisfaction and usually based on a pre-defined set of constraint primitives (such as comparison, sets operations, etc). Both paradigms are connected and the system such as \acrshort{idp} can often take best of two worlds and known as constraint logic programming. In this thesis we prefer constraint programming over logic programming for numeric-heavy problems and all other cases we use logic programming (\acrshort{asp}) or constraint logic programming (\acrshort{idp}).
\pubrevend

The essence of the \acrshort{cp} methodology consists of writing a set of
constraints, such as $X + Y > Z$, and inferring a solution to them, such
as $X = 1, Y = 2$ and $Z = 0$. This set of constraints is referred as
a \textit{specification} or a \textit{model}; for this, multiple \acrlong{cp}
languages exist, such as MiniZinc \parencite{minizinc} or Essence
\parencite{essence}. A \textit{solution} for
a model is an assignment, as for example, the one we just indicated, that satisfies
\textit{all} constraints. This general methodology is declarative,
since a user writes a set of constraints corresponding to the
properties of her problem and a \acrshort{cp} solver is looking for a
solution: this clearly demonstrates that it is a task of the solver
to figure out \textit{how} to find a solution, while it is the user's
responsibility to specify \textit{what} the problem is in terms of
writing these constraints.

\pubrev
In this thesis, we mostly focus on \textit{learning constraints} than on \textit{constraint satisfaction search}, therefore, for us, it is natural to look not only into constraint programming itself but also into the constraint learning field.
\pubrevend

\section{Constraint learning} \label{sec:intro_constraint_learning}
Constraint Learning is the research field concerned with finding
constraints that hold in data \parencite{constraint_learning,QUACQ,Conacq}.
%Let us outline in simple terms, what constraint learning is about.
Similar to the pattern matching problem, we can define 
the constraint learning problem as follows:
%Explaining what constraint learning in term of the previous section:
let $D$ be a dataset, $C$ be a set of interesting properties (constraints), and
$\phi$ be a satisfaction function, 
then the problem is to find all properties (constraints) $c$ in $C$ such that $\phi(D,c)$ is satisfied.
\begin{example}[Sudoku constraint learning]
    A well-known Sudoku problem might be hard to model for a person
    new to constraint programming. A possibility to obtain a model
    is to it learn from examples. This can be done by providing:
    \begin{itemize}
        \item A set of Sudoku solutions
        \item A set of invalid Sudoku combinations
        \item An expressive constraint language 
    \end{itemize}
Then, a property of interest is a constraint that holds for all
    positive Sudoku combinations (all solutions) and for none of
    the invalid combinations. For example, this can be a constraint of
    the form
\begin{equation*}
    \forall i: \sum_j x_{ij} = 45
\end{equation*}
    Which shows that the sum of elements in each column is equal to 45
    (simply the sum of numbers from 1 to 9).
\end{example}

In case of pattern mining, we enumerate objects on which the property
of interest holds: we find graphs or sequences that occur often enough
in the data and have certain desired topological properties (for example, being a
cyclic graph or have ascending characters). And in constraint learning we find interesting
properties that hold in the data (or its parts): given a set of
characters, for example, we discovered that the ''ascending
constraint`` holds, i.e.,
characters are sorted or we found that givens graphs are all cyclic. This shows how two problems are essentially connected.


\pubrev
\begin{figure}[thb]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{recomputed.png}
  \end{center}
  \caption{A financial report that had certain values originally computed with a software, then if we want to change a cell (in blue), we need to establish all dependencies (i.e., constraints) of this cell and then recompute values that depend upon it (indicated in red), otherwise, data becomes inconsistent with respect to its intended meaning}
  \label{fig:report_spreadsheet}
\end{figure}

Let us illustrate this idea on a real-world example of a spreadsheet. Imagine, we have downloaded a financial report in a CSV format, presented in Figure \ref{fig:report_spreadsheet}, from a business management software and wanted to change a number in it (indicated in blue). As many financial reports, this had certain values computed automatically in the software used by its original creator (who might be unavailable at the moment). Then, if we only change one value, the whole data becomes inconsistent, since there are certain dependencies in the spreadsheet to be met, e.g., the column \textbf{Total}, in the column \textbf{G}, is the sum over rows. Then, we would like to, first, discover all constraints in the data. Second, update the cell we are interested in (indicated in blue) and then, third, recompute the values (indicated in red). 

This practical problem is a clear constraint learning problem in a relational setting: we need to enumerate interesting properties, i.e., various spreadsheet constraints, having a form of first order statements and being able to carry out inference on top of a learned model and of relational data (multi-table, interconnected data containing multiple data types). And this is, indeed, the kind of problem that is problematic for classic machine learning and data mining algorithms and require relational methods we develop and investigate here.

For technical details we refer to Chapter \ref{ch:TaCLe} and the work of \textcite{tacle_demo}.
\pubrevend


\pubrev
\section{Unifying declarative logical approach}
There is a number of common traits and assumption that problems, we investigate in the thesis, share. Typical machine learning algorithm make use of the, so called, single-tuple single-table assumption. This assumption states that the data can be represented using attribute-value pairs. As we have demonstrated the problems we consider break this assumption. This indicates, the first, common feature -- they are all relational data problems. 

Second, the problems we investigate are a form of logical search and enumeration problems: we are looking for certain interesting logical expressions that hold in the data under constraints, e.g., all maximal (in size) conjunctive queries that hold in the data frequently enough.


Third, all of these problem, we investigate, are generic problems under constraints, i.e., in fact each of them is not a particular problem but a whole class of parameterized problems: discovering constraints in tabular data is not limited to only the list of specified constraints but it is designed to support the whole \textit{language} of tabular constraints, i.e., new constraints can be added there in a declarative manner in the accordance with the \textit{elaboration tolerance} principle by \textcite{elaboration_tolerance}.

Fourth, as we have just indicated, our solutions are generic and declarative in its nature, as indicated in Figure \ref{fig:declarative_schema} Once we modify the problem by adding an extra constraint or feature, we do not have to modify algorithm completely (like this often happens in the imperative approach \parencite{gspan,clospan}).

Last but not least is the way we approach them, our approach is based on declarative (constraint) logic programming. We demonstrate how these important and interesting problems can be modelled using modern logic programming systems such as \acrshort{asp} and \acrshort{idp}. Besides of purely mathematical models for the relational problems, we provide an extensive experimental evidence on the behaviour of such systems. To the best of our knowledge, this is the first empirical study of such scale investigation applications of modern logic programming to relational data mining problems.
\pubrevend

\section{Contributions}
Given the abundance of relational data (such as spreadsheets, graphs,
databases and logical structures) for various machine learning and data
mining problems, and the success in the development of declarative solvers (such
as \acrshort{asp}, \acrshort{idp}, \acrshort{cp} and \acrshort{lp}) a
natural question arises: \textit{``How can we declaratively model and
solve relational data mining problems using logic programming
techniques?''}. 

The \textbf{main contribution} of this thesis is in the application of
modern logic programming languages, such as \acrshort{asp} and
\acrshort{idp}, to these novel relational data mining problems.


Concretely, we investigate how the following data mining problems
can be modelled and solved using a declarative approach based on
logic programming.
\pubrev
\begin{itemize}
    \item \cone: What are the challenges and advantages of generalizing
    classic data mining problems, such as the Boolean Matrix
    Factorization problem, into the relation setting? We address this
        question by proposing \textbf{Relational Data Factorization.}
  \item \ctwo: How can the constraint learning problem be formulated
   and modelled in the relational setting, where data is
   represented as spreadsheets, i.e., connected relations, and constraints are
   tabular Excel-like formulae? We propose as an answer \textbf{Tabular Constraint Learning.} 
  \item \cthree: Can we model and complete a partially specified logic program?
      A partially specified program is the one where one marks certain parts of the
        program to be uncertain. We propose a declarative approach to
        this problem, called \textbf{Sketched Answer Set Programming}.
   \item \cfour:
%  Similarly to unstructured constraint-based itemset mining: 
    How can we apply declarative relational mining
    techniques, such as logic programming, to structured frequent pattern mining, such as sequence, graph
    and query mining? We introduce \textbf{Relational Pattern Mining} as a possible solution to this problem.
\end{itemize}
\pubrevend

Addressing \cone, \textbf{Chapter} \ref{ch:ReDF} presents the novel problem of Relational Data
Factorization. It demonstrates how this problem generalizes various
data mining problems such as Database Tiling and Boolean Matrix
Factorization. We start with the most basic setting and demonstrate
how various problems can be modelled within the framework by adding
or modifying the constraints. Then, we show how the framework can be
used for new and interesting applications and propose Answer Set
Programming as a method to solve the problem in the general case.
For each problem we provide extensive experimental evidence,
including solver parameter tuning. The chapter consists of the
research previously published in the following paper:

\begin{addmargin}[2em]{2em}

Sergey Paramonov,  Matthijs van Leeuwen, Luc De Raedt: Relational data
factorization, Machine Learning, Springer, December 2017, Volume 106,
    Issue 12, pp 1867–1904.

\end{addmargin}



With regards to \ctwo,  \textbf{Chapter} \ref{ch:TaCLe} introduces  the novel problem of
Tabular Constraint Learning and the system, called TaCLe (pronounced
as the word ``tackle''). The problem setting is straightforward to
explain: we have an Excel file with formulae. The file is imported as
CSV. The formulae are lost. Can we reconstruct them? We can already 
see that the problem setting is different from the standard machine
learning problem (see Section \ref{sec:intro_constraint_learning}). In a spreadsheet, columns are no longer variables
and rows are no longer records. Textual and numeric data are mixed.
Spreadsheet functions, such as Fuzzy-lookup, are unorthodox and unseen in
the constraint programming and constraint learning research communities.

The chapter consists of the research previously published in the following papers:

\begin{addmargin}[2em]{2em}
Samuel Kolb (*), Sergey Paramonov (*), Tias Guns, Luc De Raedt:
  Learning constraints in spreadsheets and tabular data. Machine
  Learning 106 (9-10), pages 1441-1468, 2017.


Sergey Paramonov (*), Samuel Kolb (*), Tias Guns, Luc De Raedt:
TaCLe: Learning constraints in tabular data. 
 Proceedings of the 2017 ACM on Conference on Information and
    Knowledge Management, CIKM 2017, pages 2511-2514, Sheridan
    Communications, Conference on Information and Knowledge Management
    (CIKM), Singapore, 6-10 Nov 2017.

\pubrev
(*) -- equal contribution.
\pubrevend
\end{addmargin}




%   we look into a more practical setting.
%   Typically, machine learning and data mining algorithms' assumptions make the i.i.d.
%   (independent identically distributed) assumption, which of course
%   breaks in the relational setting. If we look at a spreadsheet, it 
%   typically consists one or more tables with relationships between them or
%   within the table itself, usually
%   in a form of a formula, a macro or of some other constraint (e.g., a
%   C\# or VB script). If we
%   want to detect these connections between tables or their parts, we
%   have to make the model able to work with spreadsheet data, 
%   formulae and tabular constraints. This setting, referred to as the
%   \textit{Tabular constraint learning}, breaks the standard
%   machine learning and data mining algorithms and 
%   makes the problem essentially relational.

Addressing \cthree:  \textbf{Chapter} \ref{ch:sketching} introduces the novel problem of
Sketched Answer Set Programming. The idea of sketching takes root in
imperative programming, when a function in C or Java has a part of it,
typically a constant, left unspecified. A user then provides a number
of examples for this sketched constant to be filled in with a
correct value satisfying the examples. Inspired by this approach, we
introduce Sketched Answer Set Programming, where a user writes a
regular program but with certain parts such as constants, predicates
or operators are left uncertain. Then, we rewrite this sketched
program into a regular \acrlong{asp} program, i.e., we use a standard ASP solver
to complete sketches.


The chapter consists of the research previously published in the following papers:
\pubrev
\begin{addmargin}[2em]{2em}
  Sergey Paramonov, Christian Bessiere, Anton Dries, Luc De Raedt:
  Sketched Answer Set Programming. 30th International Conference on Tools with Artificial Intelligence, IEEE, 2018, International Conference On Tools With Artificial Intelligence, Volos, Greece 
\end{addmargin}
\pubrevend



%   assume we already have a logic programming or a
%   constraint model of a relational
%   data mining problem due to relational dependencies in the data which 
%   force to give up the i.i.d. assumption and, as a consequence,
%   standard techniques. When something changes over time or due to changes in the specification, we would like
%   to be able to indicate uncertainty in some parts of the model and use
%   the system itself. These
%   uncertain or, \textit{open}, parts are called \textit{sketched} and
%   the whole sketched model is referred to as a \textit{sketch} and the related
%   problem as \textit{constraint sketching}. We propose the problem of and
%   a learning technique for \acrlong{skasp}.


Finally, with respect to \cfour, \textbf{Chapter} \ref{ch:StructuredMining} introduces a logic
based approach to structured pattern mining. The idea to apply general
solvers to pattern mining takes root in the work of
\textcite{declrativeapproach}, where Constraint Programming is applied to
Itemset Mining. Contrary to itemset mining, where objects do not have
any particular structures, structured mining studies mining patterns
in sequences, trees, graphs, etc. First, we introduce a general
mathematical model of general frequent pattern mining based on the logical formulation of
the constraints. Second, we show that the model can be hybridized by
using our generic framework together with specialized solvers
developed for particular pattern mining problems.

The chapter consists of the research previously published in the following papers:
\begin{addmargin}[2em]{2em}
\pubrev
Sergey Paramonov, Daria Stepanova, Pauli Miettinen: Hybrid ASP-based
    Approach to Pattern Mining. Theory and Practice of Logic
    Programming, 2018.
\pubrevend

Sergey Paramonov, Matthijs van Leeuwen, Marc Denecker, Luc De Raedt:
An Exercise in Declarative Modelling for Relational Query Mining.
Lecture Notes in Computer Science, volume 9575, pages 166-182,
    Springer, Inductive Logic Programming, Kyoto, 20-22 August 2015.

Tias Guns, Sergey Paramonov, Benjamin Negrevergne: On declarative
    modelling of structured pattern mining, AAAI Workshop on
    Declarative Learning Based Programming, Phoenix, Arizona USA, 13
    February 2016.

Matthias van der Hallen, Sergey Paramonov, Michael Leuschel, Gerda
    Janssens: Knowledge representation analysis of graph mining,
    Bogaerts, Bart; Harrison, Amelia (eds.), Proceedings of ASPOCP
    2016, pages 55-76, Workshop on Answer Set Programming and Other
    Computing Paradigms, New York, 16 October 2016.

Sergey Paramonov, Chen Tao, Tias Guns: Generic mining of condensed
    pattern representations under constraints, Young Scientist's
    Second International Workshop on Trends in Information Processing
    (YSIP2), pages 168-177, CEUR Workshop Proceedings, Young
    Scientist's International Workshop on Trends in Information
    Processing, Dombai, Russia, 16-20 May 2017.

Sergey Paramonov, Daria Stepanova, Pauli Miettinen:
Hybrid ASP-Based Approach to Pattern Mining.  
Rules and Reasoning - International Joint Conference, RuleML+RR 2017,
    Springer Lecture Notes in Computer Science (LNCS), pages 199-214,
    Springer, 2017, RuleML+RR 2017: International Joint Conference on
    Rules and Reasoning, July 12-15, London, UK.
\end{addmargin}




%   if we look at the area of itemset mining, where \acrlong{cp}
%   has been successfully applied, then the natural question to ask is:
%   if \acrshort{cp} has been successful for unstructured, or
%   non-relational, pattern mining, can we model its \textit{structured},
%   or \textit{relational} version, using logic programming? To answer
%   this question we look into \textit{sequence}, \textit{graph} and
%   \textit{query mining} and attack them using logic programming and
%   hybrid approaches based on logic programming.

%   \paragraph{Structure of the thesis}
%   \textbf{Chapter} \ref{ch:background} introduces background information
%   on Logic, Answer Set and Programming and Pattern Mining.

\textbf{Chapter} \ref{ch:conclusions} summarizes the thesis and
discusses future work possibilities.

\section{Datasets, code and experimental results}
In order to make results repeatable and to follow the Open Source
spirit, we have opened and published the datasets, use-cases,
meta-data and other useful information.
They can be found in the following GitHub repositories:
\pubrev
\begin{itemize}
\item \url{https://github.com/SergeyParamonov/TaCLe} with materials for Chapter ''Learning Constraints in Tabular Data''
\item \url{https://github.com/SergeyParamonov/sketching} with materials for Chapter ``Sketched Answer Set Programming''
\item \url{https://github.com/SergeyParamonov/StructuredMiningASP}
    with materials for Chapter ``Relational Pattern Mining''
\item \url{https://github.com/SergeyParamonov/RelationalDataFactorization} with materials for Chapter ``Relational Data Factorization''
\end{itemize}
\pubrevend
\cleardoublepage
% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
